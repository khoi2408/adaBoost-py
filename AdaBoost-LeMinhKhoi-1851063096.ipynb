{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-08T12:50:57.409448Z","iopub.execute_input":"2022-04-08T12:50:57.409852Z","iopub.status.idle":"2022-04-08T12:50:57.422403Z","shell.execute_reply.started":"2022-04-08T12:50:57.409792Z","shell.execute_reply":"2022-04-08T12:50:57.421419Z"},"trusted":true},"execution_count":1420,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('../input/adaboostpython/spam_email.txt',sep=' ')\nonlyData=data.loc[:,'make':'cap_total']\nonlyData.describe(include='all')","metadata":{"execution":{"iopub.status.busy":"2022-04-08T12:50:57.710275Z","iopub.execute_input":"2022-04-08T12:50:57.711044Z","iopub.status.idle":"2022-04-08T12:50:57.883543Z","shell.execute_reply.started":"2022-04-08T12:50:57.710996Z","shell.execute_reply":"2022-04-08T12:50:57.882863Z"},"trusted":true},"execution_count":1421,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nX=onlyData.values\ny1=data.values[:,57]\ny=np.zeros((y1.size))\ny[y1=='ham']=1\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 0)\nstd_scaler = preprocessing.StandardScaler().fit(X_train)\nX_train_scaled = std_scaler.transform(X_train)\nX_test_scaled = std_scaler.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-04-08T12:50:57.898677Z","iopub.execute_input":"2022-04-08T12:50:57.899038Z","iopub.status.idle":"2022-04-08T12:50:57.942340Z","shell.execute_reply.started":"2022-04-08T12:50:57.898997Z","shell.execute_reply":"2022-04-08T12:50:57.941251Z"},"trusted":true},"execution_count":1422,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\npca= PCA(n_components=2)\nfig, (plt_train, plt_test) = plt.subplots(1, 2)\ndata1=pca.fit_transform(X_train_scaled)\nh1=data1[(y_train==1)]\nh2=data1[(y_train==0)]\nplt_train.set_title('Train values :'+ str(X_train_scaled.shape))\nplt_train.scatter(h1[:,0],h1[:,1],marker='*',c=\"violet\")\nplt_train.scatter(h2[:,0],h2[:,1],marker='+',c=\"orange\")\nplt_train.legend(['lable 1','lable 0'])\ndata2=pca.fit_transform(X_test_scaled)\ng1=data2[(y_test==1)]\ng2=data2[(y_test==0)]\nplt_test.set_title('Test values :'+str(X_test_scaled.shape))\nplt_test.scatter(g1[:,0],g1[:,1],marker='+',c=\"orange\")\nplt_test.scatter(g2[:,0],g2[:,1],marker='*',c=\"green\")\nplt_test.legend(['lable 1','lable 0'])","metadata":{"execution":{"iopub.status.busy":"2022-04-08T12:50:58.078974Z","iopub.execute_input":"2022-04-08T12:50:58.079309Z","iopub.status.idle":"2022-04-08T12:50:58.573606Z","shell.execute_reply.started":"2022-04-08T12:50:58.079273Z","shell.execute_reply":"2022-04-08T12:50:58.572724Z"},"trusted":true},"execution_count":1423,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\npca= PCA(n_components=2)","metadata":{"execution":{"iopub.status.busy":"2022-04-08T12:50:58.575426Z","iopub.execute_input":"2022-04-08T12:50:58.575676Z","iopub.status.idle":"2022-04-08T12:50:58.580140Z","shell.execute_reply.started":"2022-04-08T12:50:58.575643Z","shell.execute_reply":"2022-04-08T12:50:58.579137Z"},"trusted":true},"execution_count":1424,"outputs":[]},{"cell_type":"code","source":"X_scaled = std_scaler.transform(X)\ndatax=pca.fit_transform(X_scaled)\nx1=datax[(y==1)]\nx2=datax[(y==0)]\nplt.title('Train values :'+ str(X.shape))\nplt.scatter(x1[:,0],x1[:,1],marker='*',c=\"gray\")\nplt.scatter(x2[:,0],x2[:,1],marker='+',c=\"blue\")\nplt.legend(['lable 1','lable 0'])","metadata":{"execution":{"iopub.status.busy":"2022-04-08T12:50:58.581411Z","iopub.execute_input":"2022-04-08T12:50:58.581645Z","iopub.status.idle":"2022-04-08T12:50:58.951246Z","shell.execute_reply.started":"2022-04-08T12:50:58.581615Z","shell.execute_reply":"2022-04-08T12:50:58.950438Z"},"trusted":true},"execution_count":1425,"outputs":[]},{"cell_type":"code","source":"class RatingModel:\n    def __init__(self, y_, y__):\n        self.y_=y_\n        self.y__=y__\n        self.TN=np.size(y__[(y__==-1)&(y_==y__)])\n        self.FN=np.size(y__[(y__==-1)&(y_!=y__)])\n        self.TP=np.size(y__[(y__==1)&(y_==y__)])\n        self.FP=np.size(y__[(y__==1)&(y_!=y__)])\n        self.y_[self.y_==0]=-1\n        self.y__[self.y__==0]=-1\n          # assert self.y_.set={1, -1}\n          # assert self.y__.set={1, -1}\n    def __rep__():\n        return \"\"\n    def accur_Error(self, y_, y__):\n        rs=(self.TP+self.TN)/(y_.size)\n        return [rs,(1-rs)]\n    def sensitivity(self):\n        P=np.size(self.y_[self.y_==1])\n        return (self.TP)/(P)\n    def specificity(self):\n        N=np.size(self.y_[self.y_==-1])\n        return (self.TN)/(N)\n    def precision(self):\n        rs=self.TP+self.FP\n        return (self.TP)/(rs)\n    def recall(self):\n        rs=self.TP+self.FN\n        return (self.TP)/(rs)\n    def rating(self):\n        return [self.accur_Error(self.y_, self.y__), self.sensitivity(), self.specificity(), self.precision(), self.recall()]\nclass DecisionStump:\n    def __init__(self, T=100):\n        self.T = T\n        pass\n    def fit(self, X: np.ndarray, y: np.ndarray, sample_weight: np.ndarray):\n        T = self.T\n        W=sample_weight\n        nrow, ncol = X.shape\n        assert nrow == y.size\n        bestn = 0\n        bestd = 1\n        bestp = 0\n        minerr = W.sum()\n        for i in range(ncol):\n            err, d, p = self._optimize(X[:, i], y, W, T)\n            if err < minerr:\n                minerr = err\n                bestn = i\n                bestd = d\n                bestp = p\n        self.features = ncol\n        self.bestn = bestn\n        self.bestd = bestd\n        self.bestp = bestp\n        return self\n    def _optimize(self, X, y, W, T):\n        X = X.flatten()\n        min_x, max_x = X.min(), X.max()\n        len_x = max_x - min_x\n        bestd = 1\n        bestp = min_x\n        minerr = W.sum()\n        if len_x > 0.0:\n            for p in np.arange(min_x, max_x, len_x/T):\n                for d in [-1, 1]:\n                    gy = np.ones((y.size))\n                    gy[X*d < p*d] = -1\n                    err = np.sum((gy != y)*W)\n                    if err < minerr:\n                        minerr = err\n                        bestd = d\n                        bestp = p\n        return minerr, bestd, bestp\n    def predict(self, test_set : np.ndarray):\n        nrow, ncol = test_set.shape\n        assert ncol == self.features\n        icol = test_set[:, self.bestn]\n        h = np.ones((nrow))\n        h[icol*self.bestd < self.bestp*self.bestd] = -1\n        return h\nclass AdaBoost:\n    def __init__(self , T, hmodel = DecisionStump()):\n        self.T=T\n        self.hmodel=hmodel\n    def fit(self, X: np.ndarray, y_: np.ndarray, verbose=False):\n        n = X.shape[0]\n        T = self.T\n        y=y_\n        y[y==0]=-1\n        # init numpy arrays\n        self.D = np.zeros(shape=(T, n))\n        self.h = np.zeros(shape=T, dtype=object)\n        self.alpha = np.zeros(shape=T)\n        self.errors = np.zeros(shape=T)\n        self.ratting = np.zeros(shape=(T,2))\n        # initialize weights uniformly\n        self.D[0] = np.ones(shape=n) / n\n        for t in range(T):\n        # fit  weak learner\n            D_ = self.D[t]\n            h_ = DecisionStump(40)\n            h_ = h_.fit(X, y, D_)\n            # calculate error and stump weight from weak learner prediction\n            Pr_ = h_.predict(X)\n            error_ = D_[(Pr_ != y)].sum()# / n\n            alpha_ = np.log((1 - error_) / error_) / 2\n            # update sample weights\n            D_new = (\n                D_ * np.exp(-alpha_ * y * Pr_)\n            )\n            D_new /= D_new.sum()\n            # If not final iteration, update sample weights for t+1\n            if t+1 < T:\n                self.D[t+1] = D_new\n            # save results of iteration\n            self.h[t] = h_\n            self.alpha[t] = alpha_\n            self.errors[t] = error_\n            # ae=np.array([0,0])\n            if t>0:\n                Pr_temp=self.predictmodul(X,t)\n                modelra=RatingModel(y, Pr_temp)\n                self.ratting[t,:]=modelra.accur_Error(y, Pr_temp)\n            if verbose: print('{0}-th weak: accuracy={1}, error={2}'.format (t, self.ratting[t,0], self.ratting[t,1]))\n        return self\n    def predict(self, X):\n        Pr_ = np.array([h_.predict(X) for h_ in self.h])\n        return np.sign(np.dot(self.alpha, Pr_))\n    def predictmodul(self, X, i):\n        h_temp=self.h[:i]\n        alpha_temp=self.alpha[:i]\n        Pr_ = np.array([h_.predict(X) for h_ in h_temp])\n        return np.sign(np.dot(alpha_temp, Pr_))","metadata":{"execution":{"iopub.status.busy":"2022-04-08T12:50:58.953184Z","iopub.execute_input":"2022-04-08T12:50:58.953417Z","iopub.status.idle":"2022-04-08T12:50:58.989895Z","shell.execute_reply.started":"2022-04-08T12:50:58.953387Z","shell.execute_reply":"2022-04-08T12:50:58.989082Z"},"trusted":true},"execution_count":1426,"outputs":[]},{"cell_type":"code","source":"model=AdaBoost(70)\nmodel=model.fit(X_train_scaled, y_train,  True )\nPr=model.predict( X_test_scaled)\nPr[(Pr==0)]=-1\nprint(Pr, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-04-08T12:50:58.991182Z","iopub.execute_input":"2022-04-08T12:50:58.991414Z","iopub.status.idle":"2022-04-08T12:51:14.207934Z","shell.execute_reply.started":"2022-04-08T12:50:58.991386Z","shell.execute_reply":"2022-04-08T12:51:14.206884Z"},"trusted":true},"execution_count":1427,"outputs":[]},{"cell_type":"code","source":"ra_Xtest = np.zeros(shape=(model.T,2))\nfor i in range(1,model.T):\n    Pr_i=model.predictmodul(X_test_scaled,i)\n    modelra=RatingModel(y_test, Pr_i)\n    ra_Xtest[i,:]=modelra.accur_Error(y_test, Pr_i)\nra_Xtrain = np.zeros(shape=(model.T,2))\nfor i in range(1,model.T):\n    Pr_i=model.predictmodul(X_train_scaled,i)\n    modelra=RatingModel(y_train, Pr_i)\n    ra_Xtrain[i,:]=modelra.accur_Error(y_train, Pr_i)\niter=range(model.T)\nplt.plot(iter,ra_Xtest[:,0],'y-', label='Test accuracy')\nplt.plot(iter,ra_Xtest[:,1],'r-', label='Test error')\nplt.plot(iter,ra_Xtrain[:,0],'y--', label='Train accuracy')\nplt.plot(iter,ra_Xtrain[:,1],'r--', label='Train error')\nplt.legend(loc='center right')\nplt.xlabel('Iter')\nplt.ylabel('Loss/Accuracy')","metadata":{"execution":{"iopub.status.busy":"2022-04-08T12:51:14.210047Z","iopub.execute_input":"2022-04-08T12:51:14.210352Z","iopub.status.idle":"2022-04-08T12:51:14.778828Z","shell.execute_reply.started":"2022-04-08T12:51:14.210307Z","shell.execute_reply":"2022-04-08T12:51:14.777935Z"},"trusted":true},"execution_count":1428,"outputs":[]},{"cell_type":"code","source":"sumerror=0;\ny_new=y_test\ny_new[y_new==0]=-1\nfor i in range(y_new.shape[0]):\n    if y_new[i]!=Pr[i]: \n        sumerror+=1\nprint(sumerror, y_new.shape)\ngT1=data2[(Pr==1)]\ngT0=data2[(Pr==-1)]\ngF1=data2[(y_new!=Pr)&(Pr==1)]\ngF0=data2[(y_new!=Pr)&(Pr==-1)]\nplt.title('Test values errors :'+str(sumerror)+'/ '+str(X_test_scaled.shape[0]))\n# plt.scatter(gF[:,0],gF[:,1], c=\"red\")\nplt.scatter(gT1[:,0],gT1[:,1], marker='*')\nplt.scatter(gT0[:,0],gT0[:,1], marker='x')\nplt.scatter(gF1[:,0],gF1[:,1], marker='s')\nplt.scatter(gF0[:,0],gF0[:,1], marker='+')\nplt.legend(['TP C','TN G','FP C','FN G'])   ","metadata":{"execution":{"iopub.status.busy":"2022-04-08T12:51:14.780005Z","iopub.execute_input":"2022-04-08T12:51:14.780229Z","iopub.status.idle":"2022-04-08T12:51:15.037710Z","shell.execute_reply.started":"2022-04-08T12:51:14.780202Z","shell.execute_reply":"2022-04-08T12:51:15.036838Z"},"trusted":true},"execution_count":1429,"outputs":[]}]}